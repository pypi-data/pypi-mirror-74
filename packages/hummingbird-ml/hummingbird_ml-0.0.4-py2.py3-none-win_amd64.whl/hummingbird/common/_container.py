# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
# --------------------------------------------------------------------------

import torch
from hummingbird.common.data_types import TensorListType


class RawModelContainerNode(object):
    """
    This node is the carrier of the model we want to convert.
    It provides an abstract layer so that our parsing
    framework can work with models generated by different tools.
    """

    def __init__(self, raw_model):
        """
        :param raw_model: *scikit-learn* model to convert
        """
        self._raw_model = raw_model

    @property
    def raw_model(self):
        return self._raw_model

    @property
    def input_names(self):
        """
        This function should return a list of strings. Each string
        corresponds to an input variable name.
        :return: a list of string
        """
        raise NotImplementedError()

    @property
    def output_names(self):
        """
        This function should return a list of strings. Each string
        corresponds to an output variable name.
        :return: a list of string
        """
        raise NotImplementedError()


class SklearnModelContainerNode(RawModelContainerNode):
    """
    Main container for one *scikit-learn* model.
    Every converter adds nodes to an existing container
    which is converted into a *PyTorch* graph by an instance of
    :class:`Topology <hummingbird.common._topology.Topology>`.
    """

    def __init__(self, sklearn_model):
        super(SklearnModelContainerNode, self).__init__(sklearn_model)
        # Scikit-learn models have no input and output specified,
        # so we create them and store them in this container.
        self._inputs = []
        self._outputs = []

    @property
    def input_names(self):
        return [variable.raw_name for variable in self._inputs]

    @property
    def output_names(self):
        # Raw names are non unique. Hence using pytorch names.
        return [variable.pytorch_name for variable in self._outputs]

    def add_input(self, variable):
        # The order of adding variables matters. The final model's
        # input names are sequentially added as this list
        if variable not in self._inputs:
            self._inputs.append(variable)

    def add_output(self, variable):
        # The order of adding variables matters. The final model's
        # output names are sequentially added as this list
        if variable not in self._outputs:
            self._outputs.append(variable)


class Skl2PyTorchModel(torch.nn.Module):

    def __init__(
            self,
            input_names,
            output_names,
            operator_map,
            topology,
            device,
            extra_config):
        super(Skl2PyTorchModel, self).__init__()
        self.input_names = input_names
        self.output_names = output_names
        self.operator_map = torch.nn.ModuleDict(operator_map)
        self.topology = topology
        self.device = device
        self.extra_config = extra_config

    def is_jittable(self, *pytorch_inputs):
        return 'SklearnSVC' not in self.operator_map.keys(
        ) and 'SklearnNuSVC' not in self.operator_map.keys()

    def forward(self, *pytorch_inputs):
        with torch.no_grad():
            pytorch_inputs = [*pytorch_inputs]
            variable_map = {}
            for i, input_name in enumerate(self.input_names):
                if self.device is not None and not isinstance(
                        self.topology.variables[input_name].type, TensorListType):
                    pytorch_inputs[i] = pytorch_inputs[i].to(self.device)
                variable_map[input_name] = pytorch_inputs[i]

            for operator in self.topology.topological_operator_iterator():
                pytorch_op = self.operator_map[operator.full_name]
                pytorch_outputs = pytorch_op(
                    *(variable_map[input.pytorch_name] for input in operator.inputs))

                if len(operator.outputs) == 1:
                    variable_map[operator.outputs[0].pytorch_name] = pytorch_outputs
                else:
                    for i, output in enumerate(operator.outputs):
                        variable_map[output.pytorch_name] = pytorch_outputs[i]

            if len(self.output_names) == 1:
                return variable_map[self.output_names[0]]
            else:
                return list(variable_map[output_name]
                            for output_name in self.output_names)
