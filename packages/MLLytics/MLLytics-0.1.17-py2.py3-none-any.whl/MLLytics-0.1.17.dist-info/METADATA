Metadata-Version: 2.1
Name: MLLytics
Version: 0.1.17
Summary: A library of tools for easier evaluation of ML models.
Home-page: https://github.com/scottclay/MLLytics
Author: Scott Clay
Author-email: scottclay8@gmail.com
License: MIT
Platform: UNKNOWN
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Topic :: Software Development
Classifier: Topic :: Scientific/Engineering
Classifier: Operating System :: Microsoft :: Windows
Classifier: Operating System :: Unix
Classifier: Operating System :: MacOS
Description-Content-Type: text/markdown
Requires-Dist: numpy (>=1.14.3)
Requires-Dist: matplotlib (>=2.2.2)
Requires-Dist: seaborn (>=0.8.1)
Requires-Dist: pandas (>=0.23.0)
Requires-Dist: scikit-learn (>=0.19.1)

# MLLytics

## Installation instructions
```pip install MLLytics```
or
```python setup.py install```
or
``` conda env create -f environment.yml```

## Update pypi instructions (for me)
Creates the package
```python setup.py sdist bdist_wheel```
Upload package
```twine upload --repository pypi *version_files*```

## Future
### Improvements and cleanup
* Allow figure size and font sizes to be passed into plotting functions
* Comment all functions and classes
* Add type hinting to all functions and classes (https://mypy.readthedocs.io/en/latest/cheat_sheet_py3.html)
* Example guides for each function in jupyter notebooks
* ~MultiClassMetrics should inherit from ClassMetrics~
* REGRESSION

### Cosmetic
* Fix size of confusion matrix
* Check works with matplotlib 3
* Tidy up legends and annotation text on plots
* Joy plots
* Brier score for calibration plot
* Tidy up cross validation and plots (also repeated cross-validation)
* Acc-thresholds graph



### Big push
* Scoring functions
* MultiClassMetrics class to inherit from ClassMetrics and share common functions
* More output stats in overviews
* Update reliability plot https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/
* Tests
* Switch from my metrics to sklearn metrics where it makes sense? aka
```fpr, tpr, thresholds = roc_curve(y[test], probas_[:, 1])```
and more general macro/micro average metrics from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score
* Additional metrics (sensitivity, specificity, precision, negative predictive value, FPR, FNR,
false discovery rate, accuracy, F1 score


## Contributing Authors
* Scott Clay
* David Sullivan


