# -*- coding: utf-8 -*-
# Copyright Â© 2020 Contrast Security, Inc.
# See https://www.contrastsecurity.com/enduser-terms-0317a for more details.
import hashlib
import json
import os
import time
from io import open
from multiprocessing import cpu_count
from multiprocessing.dummy import Pool

import pip
import pkg_resources

from contrast.api.dtm_pb2 import Library
from contrast.utils.patch_utils import get_loaded_modules

import logging

logger = logging.getLogger("contrast")


def get_active_library_names_from_pkg():
    library_names = ["contrast"]
    libraries = list(pkg_resources.working_set)
    for library in libraries:
        library_names.append(library.key)
    return library_names


class LibraryReader(object):
    def __init__(self, library_tags=""):
        self.installed_distribution_keys = []

        self.libraries = []
        self.library_tags = (
            library_tags if isinstance(library_tags, str) else ",".join(library_tags)
        )

        self.top_level_modules = dict()
        self.unused_libraries = []

    def read_libraries(self):
        """
        Looks at every library installed in self.installed_dists, then calls search_dist
        on each dist that has a file top_level.txt, which is autogenerated by installation,
        specifying which files/directories are associated with the package

        :exception: IOError if top_level.txt, METADATA, or RECORD cannot be found
        :return: None
        """
        logger.debug("Analyzing libraries... ")

        # All packages installed in the site-packages/ directory
        installed_dists = LibraryReader.get_installed_distributions()

        if not installed_dists:
            self.libraries = []
            self.unused_libraries = []
            return

        packages = []
        unused_packages = []

        all_dist_keys = [dist._key for dist in installed_dists]

        self.installed_distribution_keys = [
            x for x in all_dist_keys if x != CONTRAST_AGENT_DIST
        ]

        logger.debug("Found %s libraries", len(self.installed_distribution_keys))

        results = read_dists(installed_dists)

        for found, dist_dict in results:
            library_name = dist_dict.get("file_path", "")

            if library_name and library_name != CONTRAST_AGENT_DIST:
                if found:
                    packages.append(dist_dict)
                else:
                    unused_packages.append(dist_dict)

        logger.debug(
            "Used libraries: %s", [lib.get("file_path", "") for lib in packages]
        )
        logger.debug(
            "Unused libraries: %s",
            [lib.get("file_path", "") for lib in unused_packages],
        )

        libraries = [
            convert_dict_to_library(lib, self.library_tags, self.top_level_modules)
            for lib in packages
        ]
        unused_libraries = [
            convert_dict_to_library(lib, self.library_tags, self.top_level_modules)
            for lib in unused_packages
        ]

        self.libraries = libraries
        self.unused_libraries = unused_libraries

    @staticmethod
    def get_installed_distributions():
        pip_version = pip.__version__
        major_version = int(pip_version.split(".")[0])

        # newest versions of pip
        if major_version >= 10:  # ['10', '18', '19']
            from pip._internal.utils.misc import get_installed_distributions

            return get_installed_distributions()

        if 6 <= major_version <= 9:  # ['6', '7', '8', '9']
            # 9 and below; these releases go back until Dec 22, 2014
            return pip.get_installed_distributions()

        return []


def read_dists(*args):
    """
    Pool is for processes, ThreadPool is just threads. Both are fast
    """
    pool = Pool(processes=cpu_count())

    results = pool.map(search_dist, args[0])

    pool.close()
    pool.join()

    return results


def convert_dict_to_library(lib, library_tags, top_level_modules):
    """
    DTM objects are not pickle-able for the the multiprocess library;
    we'll return a dict from search_dist then convert to DTM Library
    :param top_level_modules: save these for after the process returns
    :param lib: library dict
    :param library_tags: tags to add to each library
    :return: Library DTM object
    """

    top_level_modules[lib["file_path"]] = lib["top_level_modules"]

    current_time = int(time.time() * 1000)

    library = Library()
    library.version = lib["version"]
    library.manifest = lib["manifest"]
    library.class_count = lib["class_count"]
    library.used_class_count = lib["used_class_count"]
    library.file_path = lib["file_path"]
    library.url = lib["url"]
    library.hash_code = lib["hash_code"]
    library.external_ms = current_time
    library.internal_ms = current_time

    if library_tags:
        library.tags = library_tags

    return library


CONTRAST_AGENT_DIST = "contrast-agent"
RECORD = "RECORD"
TOP_LEVEL_TXT = "top_level.txt"

PY_SUFFIX = ".py"
SO_SUFFIX = ".so"


def _get_top_level_directories(dist):
    # some packages have multiple top level directories, so check for all of them

    top_level_dirs = []

    # file storing directory names for top level directories of package
    if dist.has_metadata(TOP_LEVEL_TXT):
        top_level_dirs = list(dist.get_metadata_lines(TOP_LEVEL_TXT))
    elif dist.has_metadata(RECORD):
        metadata_lines = [d.split(",")[0] for d in dist.get_metadata_lines(RECORD)]

        top_level_dirs = [
            x
            for x in metadata_lines
            if "/" in x and x.split(os.sep)[1] == "__init__.py"
        ]
    else:
        if isinstance(dist._provider, pip._vendor.pkg_resources.FileMetadata):
            path = "/".join([dist._provider.path, TOP_LEVEL_TXT])
        else:
            # pip._vendor.pkg_resources.PathMetadata
            path = "/".join([dist._provider.egg_info, TOP_LEVEL_TXT])

        try:
            with open("/".join([path, TOP_LEVEL_TXT]), "rb") as tl:
                top_level_dirs = tl.readlines()
        except IOError as _:
            pass

    return top_level_dirs


def search_dist(args):
    """
    Searches directories related to dist, gathering relevant statistics and metadata for LibraryDTM.
    Then, assuming library was loaded, appends that metadata to the self.libraries in a LibraryDTM

    Created package is added to the output for the process

    :param args: contains a DistInfoDistribution object referencing the current library being looked for
    :return: None
    """
    dist = args
    version = manifest = url = ""
    total_files = set()
    total_used_files = set()

    # Variable tracking if the package has been found in sys.modules
    package_found = False

    top_level_dirs = _get_top_level_directories(dist)

    for directory in top_level_dirs:
        package_path = "/".join([dist.location, directory])

        all_dir_files = get_all_files(package_path)

        if package_found:
            dir_used_files = search_modules(
                package_found, package_path, dist, directory
            )[0]
        else:
            dir_used_files, package_found, version, manifest, url = search_modules(
                package_found, package_path, dist, directory
            )

        compiled_files = []
        if len(dir_used_files) > 0:
            try:
                # increment for .so files; we assume if other files are used then compiled files are wrapped
                compiled_files = count_compiled_files(directory, dist, package_path)
            except:
                pass

        for x in all_dir_files + dir_used_files + compiled_files:
            total_files.add(x)

        for x in dir_used_files + compiled_files:
            total_used_files.add(x)

    file_count = len(total_files)
    used_file_count = len(total_used_files)

    # Set the package path to be the first directory mentioned in top_level.txt
    name = str(dist).split(" ")[0]
    library_hash = get_hash(name, version)

    # tuple of if we found it on the path and the package object
    # we only report those on the path
    result = (
        package_found,
        create_package(
            version,
            manifest,
            file_count,
            used_file_count,
            name,
            url,
            library_hash,
            top_level_dirs,
        ),
    )

    return result


def create_package(
    version, manifest, class_count, used_class_count, name, url, sha, top_level_dirs
):
    """
    Add package with appropriate metadata to the library; Reason for this is that protobuf cannot be pickled
    :param version: version of package
    :param manifest: all metadata
    :param class_count: count of files
    :param used_class_count: count of used/loaded files
    :param name: name of the library
    :param url: homepage of package
    :param sha: sha1 hash of the name(space)version
    :param top_level_dirs: names of the first level of modules
    :return: dict of package
    """
    current_time = int(time.time() * 1000)

    return {
        "version": version,
        "manifest": manifest,
        "class_count": class_count,
        "used_class_count": used_class_count,
        "file_path": name,
        "url": url,
        "hash_code": sha,
        "external_ms": current_time,
        "internal_ms": current_time,
        "tags": "",
        "top_level_modules": top_level_dirs,
    }


def search_modules(package_found, package_path, dist, directory_name):
    """
    Searches every module to see if it is associated with the current package being looked at
    If the package hasn't been found, it gets the metadata for the package, otherwise it
    just keeps updating used file count

    :param directory_name: name of top level directory
    :param package_found: Variable tracking if the package has been found
    :param package_path: Path to the package currently being looked for
    :param dist: The distribution object for the library
    :return: The list of used files for this directory, package_found, name, version, manifest, and url
    name, version, manifest, and url are empty strings if the package was already found
    """

    used_files = []
    version = manifest = url = ""

    sys_modules = get_loaded_modules().values()

    filtered_modules = list(
        {
            x
            for x in sys_modules
            if (x.__name__ + ".").startswith(directory_name)
            or package_path in x.__name__
            or package_path in str(x)
        }
    )

    for sys_module in filtered_modules:

        # if the package path is in the module path, the package is loaded
        if not package_found and hasattr(sys_module, "__path__"):
            sys_module_path = sys_module.__path__

            if sys_module_path:
                any_path = (
                    any(
                        [
                            package_path.startswith(mod_path)
                            for mod_path in sys_module_path
                        ]
                    )
                    or package_path in sys_module_path
                )

                if any_path:
                    (version, manifest, url) = get_data(dist)
                    package_found = True

        # If the package has been found, and sys_module's file is in package directory
        if package_found and hasattr(sys_module, "__file__"):
            file_path = sys_module.__file__

            try:
                if package_path in file_path:
                    # Increment count of used files
                    used_files.append(file_path)
            except:
                for namespace in [l for l in dir(file_path) if not l.startswith("__")]:
                    if package_path in file_path[namespace]:
                        # Increment count of used files
                        used_files.append(file_path)

    if not package_found and len(used_files) > 0:
        package_found = True
        (version, manifest, url) = get_data(dist)

    return used_files, package_found, version, manifest, url


def count_compiled_files(directory_name, dist, package_path):
    if not os.path.exists(package_path):
        try:
            files = list(dist.get_metadata_lines(RECORD))
        except IOError:
            files = list(dist.get_metadata_lines(dist.PKG_INFO))

        compiled_files = [
            x for x in files if x.startswith(directory_name) and SO_SUFFIX in x
        ]
        return compiled_files

    return []


def get_all_files(package_path):
    """
    Get all .py and .so files in all directories in package_path.
    :param package_path: Path to the current package being looked at
    :return: list of files
    """
    if not os.path.exists(package_path):
        return []

    total_files = []
    if os.path.isdir(package_path):
        for _, _, files in os.walk(package_path):
            for f in files:
                if f.endswith((PY_SUFFIX, SO_SUFFIX)):
                    total_files.append(f)

    # check for one file in site-packages
    if os.path.exists(package_path + PY_SUFFIX):
        total_files.append(package_path + PY_SUFFIX)

    return total_files


def get_data(dist):
    """
    Given a dist, pulls name, version, manifest, and url out of the metadata
    :param dist: the distribution package whose package info is being retrieved
    :return: the package info from the metadata
    """
    version = dist.version
    manifest = dist.get_metadata(dist.PKG_INFO)

    # If the data has a .json format
    if dist.PKG_INFO == "METADATA":
        url = get_metadata(dist)

    # If the data is in PKG-INFO form
    elif dist.PKG_INFO == "PKG-INFO":
        url = get_pkg_info(dist)

    # If the metadata is not in either form
    else:
        raise Exception(dist.PKG_INFO, "Package metadata not found")

    return version, manifest, str(url)


EXTENTIONS = "extensions"
HOME = "Home"
PROJECT_URLS = "project_urls"
PYTHON_DETAILS = "python.details"


def get_metadata(dist):
    """
    Gets the library data if PKG_INFO is packaged in a json
    :param dist: the distribution package who's data is being parsed
    :return: the url of the package
    """

    try:
        metadata = json.loads(dist.get_metadata("metadata.json"))

        if PROJECT_URLS in metadata[EXTENTIONS][PYTHON_DETAILS]:
            return metadata[EXTENTIONS][PYTHON_DETAILS][PROJECT_URLS][HOME]
    except:
        pass

    try:
        metadata = json.loads(dist.get_metadata("pydist.json"))
        if PROJECT_URLS in metadata:
            return metadata[PROJECT_URLS][HOME]
    except:
        pass

    return ""


HOME_PAGE = "Home-page: "


def get_pkg_info(dist):
    """
    Gets the library data if PKG_INFO is packaged in a text file
    :param dist: the distribution package who's data is being parsed
    :return: the url of the package
    """

    # Split metadata so it can be searched
    metadata = list(dist.get_metadata_lines(dist.PKG_INFO))

    # Search metadata for homepage
    for line in metadata:
        if line.startswith(HOME_PAGE):
            return line.split(HOME_PAGE)[1]

    return ""


def get_hash(name, version):
    """
    DO NOT ALTER OR REMOVE
    """
    to_hash = name + " " + version

    return hashlib.sha1(to_hash.encode("utf-8")).hexdigest()
